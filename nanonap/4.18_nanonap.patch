From d23c6a8602f90f5a61168493a0c7019b15b5df8c Mon Sep 17 00:00:00 2001
From: Makiras <makiras.x@outlook.com>
Date: Sun, 31 Oct 2021 16:48:44 +0800
Subject: [PATCH] finish nanonap patch

---
 arch/x86/entry/common.c      |  3 +++
 arch/x86/kernel/process_64.c |  2 ++
 kernel/sched/core.c          |  8 +++++++-
 kernel/sched/idle.c          | 20 ++++++++++++++++++++
 4 files changed, 32 insertions(+), 1 deletion(-)

diff --git a/arch/x86/entry/common.c b/arch/x86/entry/common.c
index 3b2490b8..6d71035f 100644
--- a/arch/x86/entry/common.c
+++ b/arch/x86/entry/common.c
@@ -269,6 +269,7 @@ __visible inline void syscall_return_slowpath(struct pt_regs *regs)
 }
 
 #ifdef CONFIG_X86_64
+DECLARE_PER_CPU(int, shim_curr_syscall);
 __visible void do_syscall_64(unsigned long nr, struct pt_regs *regs)
 {
 	struct thread_info *ti;
@@ -285,12 +286,14 @@ __visible void do_syscall_64(unsigned long nr, struct pt_regs *regs)
 	 * regs->orig_ax, which changes the behavior of some syscalls.
 	 */
 	nr &= __SYSCALL_MASK;
+	__this_cpu_write(shim_curr_syscall, (int)nr);
 	if (likely(nr < NR_syscalls)) {
 		nr = array_index_nospec(nr, NR_syscalls);
 		regs->ax = sys_call_table[nr](regs);
 	}
 
 	syscall_return_slowpath(regs);
+	__this_cpu_write(shim_curr_syscall, -1);
 }
 #endif
 
diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 12bb445f..714f11fb 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -60,6 +60,8 @@
 #endif
 
 __visible DEFINE_PER_CPU(unsigned long, rsp_scratch);
+__visible DEFINE_PER_CPU(int, shim_curr_syscall);
+EXPORT_PER_CPU_SYMBOL(shim_curr_syscall);
 
 /* Prints also some state that isn't saved in the pt_regs */
 void __show_regs(struct pt_regs *regs, int all)
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index fe365c9a..fd989709 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -2994,9 +2994,15 @@ void sched_exec(void)
 
 DEFINE_PER_CPU(struct kernel_stat, kstat);
 DEFINE_PER_CPU(struct kernel_cpustat, kernel_cpustat);
+DEFINE_PER_CPU(unsigned long, shim_curr_task);
+DEFINE_PER_CPU(unsigned long, shim_sleep_flag);
+DEFINE_PER_CPU(unsigned long *, shim_wakeup_ptr);
 
 EXPORT_PER_CPU_SYMBOL(kstat);
 EXPORT_PER_CPU_SYMBOL(kernel_cpustat);
+EXPORT_PER_CPU_SYMBOL(shim_curr_task);
+EXPORT_PER_CPU_SYMBOL(shim_sleep_flag);
+EXPORT_PER_CPU_SYMBOL(shim_wakeup_ptr);
 
 /*
  * The function fair_sched_class.update_curr accesses the struct curr
@@ -3496,7 +3502,7 @@ static void __sched notrace __schedule(bool preempt)
 		++*switch_count;
 
 		trace_sched_switch(preempt, prev, next);
-
+		__this_cpu_write(shim_curr_task, (unsigned long)task_pid_nr(next) | ((unsigned long)task_tgid_nr(next)<<32));
 		/* Also unlocks the rq: */
 		rq = context_switch(rq, prev, next, &rf);
 	} else {
diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 1a3e9bdd..c08b9c76 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -216,6 +216,9 @@ static void cpuidle_idle_call(void)
 	rcu_idle_exit();
 }
 
+DECLARE_PER_CPU(unsigned long, shim_sleep_flag);
+DECLARE_PER_CPU(unsigned long*, shim_wakeup_ptr);
+
 /*
  * Generic idle loop implementation
  *
@@ -237,6 +240,8 @@ static void do_idle(void)
 	tick_nohz_idle_enter();
 
 	while (!need_resched()) {
+		unsigned long *shim_target_flag = __this_cpu_read(shim_wakeup_ptr);
+		*shim_target_flag  = 0xdead;
 		check_pgt_cache();
 		rmb();
 
@@ -347,6 +352,8 @@ EXPORT_SYMBOL_GPL(play_idle);
 
 void cpu_startup_entry(enum cpuhp_state state)
 {
+	int my_cpu, target_cpu, nr_cpu;
+	unsigned long *shim_target_wait;
 	/*
 	 * This #ifdef needs to die, but it's too late in the cycle to
 	 * make this generic (ARM and SH have never invoked the canary
@@ -362,6 +369,19 @@ void cpu_startup_entry(enum cpuhp_state state)
 	 */
 	boot_init_stack_canary();
 #endif
+    my_cpu = smp_processor_id();
+    nr_cpu = num_possible_cpus();
+	
+    if (my_cpu < nr_cpu/2)
+    	target_cpu = my_cpu + nr_cpu/2;
+    else
+    	target_cpu = my_cpu - nr_cpu/2;
+	
+
+    shim_target_wait = per_cpu_ptr(&shim_sleep_flag, target_cpu);
+    __this_cpu_write(shim_wakeup_ptr, shim_target_wait);
+    // printk(KERN_DEBUG "SHIM:idle starts on cpu %d, neighbour cpu %d, waits on %p, neighbour on %p\n", my_cpu, target_cpu, &shim_wakeup_ptr, shim_target_wait);
+
 	arch_cpu_idle_prepare();
 	cpuhp_online_idle(state);
 	while (1)
-- 
2.25.1

